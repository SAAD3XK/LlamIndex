{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "# from llama_index.llms import LlamaCPP\n",
    "# from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "other_cpp = Llama(model_path='/home/saadsameerkhan/Downloads/llama-2-13b.Q5_K_M.gguf', n_ctx=800)\n",
    "prompt = \"Q: What are you? A:\"\n",
    "output = other_cpp(prompt, max_tokens=0)\n",
    "cpp_llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path='/home/saadsameerkhan/Downloads/llama-2-13b.Q5_K_M.gguf',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 0},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/saadsameerkhan/Downloads/\",\n",
    "    model_file='llama-2-13b.Q5_K_M.gguf',\n",
    "    model_type=\"llama\",\n",
    "    gpu_layers=0\n",
    "    )\n",
    "\n",
    "model_name = 'llama-2-7b-chat.Q5_K_S.gguf'\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
    "    model_file=model_name,\n",
    "    model_type=\"llama\",\n",
    "    gpu_layers=0\n",
    "    )\n",
    "\n",
    "for text in other_cpp(\"AI is going to\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
